# Tu Primer Agente RAG: Construye un Asistente IA que Lee PDFs

## Â¿QuÃ© vas a construir?

Vas a crear tu primer agente de inteligencia artificial capaz de **leer documentos PDF y responder preguntas** sobre su contenido. Es como tener un asistente personal que lee por ti y te explica lo que encuentra.

**Tiempo estimado:** 45-60 minutos

---

## Requisitos previos

âœ… Python 3.10 o superior instalado  
âœ… Un editor de cÃ³digo (VS Code recomendado)  
âœ… API Key de OpenAI ([obtenerla aquÃ­](https://platform.openai.com))  
âœ… Un PDF para probar (cualquier documento)

---

## PASO 0: Instala uv (el manejador de paquetes moderno)

### Â¿QuÃ© es uv?

Es un manejador de paquetes de Python **ultra rÃ¡pido** creado por Astral (los creadores de Ruff). Es hasta 100x mÃ¡s rÃ¡pido que pip tradicional.

### Instala uv:

**Windows (PowerShell):**

```powershell
powershell -c "irm https://astral.sh/uv/install.ps1 | iex"
```

**Mac/Linux:**

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

### Verifica la instalaciÃ³n:

```bash
uv --version
```

**DeberÃ­as ver algo como: `uv 0.5.x`**

---

## PASO 1: Configura tu proyecto con uv

```bash
# Crea la carpeta del proyecto
mkdir mi-primer-agente-rag
cd mi-primer-agente-rag

# Inicializa el proyecto con uv
uv init
```

![/images/create-project-uv.gif](/images/create-project-uv.gif)

**Esto crea automÃ¡ticamente:**

- Un entorno virtual en `.venv`
- Un archivo `pyproject.toml` para manejar dependencias
- Un archivo `.python-version` con la versiÃ³n de Python

---

## PASO 2: Instala las librerÃ­as necesarias con uv

### Â¿QuÃ© harÃ¡s?

Instalar todas las herramientas que tu agente necesita usando uv.

### Â¿Por quÃ© cada una?

- **langchain**: Framework para construir aplicaciones de IA
- **langchain-openai**: ConexiÃ³n con GPT
- **chromadb**: Base de datos vectorial (el "cerebro" de tu agente)
- **pypdf**: Lector de archivos PDF
- **python-dotenv**: Manejo seguro de tu API Key

### Comando:

```bash
uv add langchain langchain-openai chromadb pypdf python-dotenv
```

![Install dependencies](/images/install-dep.gif)

---

## PASO 3: Protege tu API Key

### Â¿QuÃ© harÃ¡s?

Crear un archivo `.env` para guardar tu API Key de forma segura.

### Â¿Por quÃ©?

**Nunca debes compartir tu API Key pÃºblicamente.** Usar `.env` evita que accidentalmente la subas a GitHub.

### Crea o abre .gitignore

`.gitignore`

```bash
.env
chroma_db/
.venv
```

![crea gitignore](/images/create-gitignore.gif)

Tu archivo debe verse asi:

### Crea o abre `.env` y agrega:

```
OPENAI_API_KEY=tu-api-key-aqui
```

si no sabes como crear tu api key aca te dejo un [video](https://www.youtube.com/watch?v=um4jXio7NjQ)

![env openai](/images/env-openai.gif)

**Guarda el archivo. Tu API Key ahora estÃ¡ protegida.**

---

## PASO 4: Importa las librerÃ­as

### Â¿QuÃ© harÃ¡s?

Crear el archivo `agente_rag.py` e importar todas las herramientas.

### Â¿Por quÃ© cada import?

- `os` y `dotenv`: Para leer la API Key del archivo `.env`
- `OpenAIEmbeddings`: Convierte texto en vectores (nÃºmeros)
- `ChatOpenAI`: El modelo GPT que responderÃ¡ preguntas
- `PyPDFLoader`: Lee archivos PDF
- `RecursiveCharacterTextSplitter`: Divide documentos grandes en pedazos
- `Chroma`: La base de datos vectorial
- `RetrievalQA`: La cadena RAG completa

### CÃ³digo:

```python
import os
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
```

---

## PASO 5: Carga tu API Key

### Â¿QuÃ© harÃ¡s?

Leer la API Key del archivo `.env` de forma segura.

### Â¿Por quÃ©?

Necesitas autenticarte con OpenAI para usar GPT. Sin esto, el agente no funcionarÃ¡.

### CÃ³digo:

```python
# Cargar variables del archivo .env
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

# Verificar que existe
if not api_key:
    print("âŒ Error: No se encontrÃ³ OPENAI_API_KEY en el archivo .env")
    exit()

print("âœ… API Key cargada correctamente")
```

**Esto previene errores si olvidaste configurar el `.env`.**

---

## PASO 6: Carga el PDF

### CÃ³digo:

```python
print("\nğŸ“„ Cargando el PDF...")
ruta_pdf = input("Escribe la ruta de tu PDF: ")

try:
    loader = PyPDFLoader(ruta_pdf)
    documentos = loader.load()
    print(f"âœ… PDF cargado: {len(documentos)} pÃ¡ginas encontradas")
except Exception as e:
    print(f"âŒ Error al cargar el PDF: {e}")
    exit()
```

## PASO 7: Divide el texto en chunks

### CÃ³digo:

```python
print("\nâœ‚ï¸ Dividiendo el documento en partes mÃ¡s pequeÃ±as...")
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
chunks = text_splitter.split_documents(documentos)
print(f"âœ… Documento dividido en {len(chunks)} chunks")
```

- `chunk_size=1000`: Cada pedazo tiene mÃ¡ximo 1000 caracteres
- `chunk_overlap=200`: Hay 200 caracteres repetidos entre chunks para no perder contexto

**Un PDF de 5 pÃ¡ginas puede generar 20-30 chunks.**

**PyPDFLoader extrae el texto de cada pÃ¡gina y crea un "documento" por pÃ¡gina.**

---

## PASO 8: Crea embeddings y la base de datos vectorial

### Â¿QuÃ© harÃ¡s?

Convertir cada chunk en un vector (lista de nÃºmeros) y guardarlo en ChromaDB.

### Â¿Por quÃ©?

Los embeddings representan el **significado** del texto como nÃºmeros. ChromaDB busca chunks con significados similares a tu pregunta usando matemÃ¡ticas.

**Ejemplo:** "Python programming" y "codificaciÃ³n en Python" tienen vectores muy parecidos aunque las palabras sean diferentes.

### CÃ³digo:

```python
print("\nğŸ§  Creando la base de conocimiento...")
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory="./chroma_db"
)
print("âœ… Base de conocimiento creada")
```

**Esto toma 10-30 segundos. Se crea una carpeta `chroma_db` con tu base de datos.**

---

## PASO 9: Inicializa el modelo GPT

### Â¿QuÃ© harÃ¡s?

Configurar el modelo de lenguaje que responderÃ¡ las preguntas.

### Â¿Por quÃ© estos parÃ¡metros?

- `model="gpt-3.5-turbo"`: Modelo rÃ¡pido y econÃ³mico (puedes usar gpt-4 si quieres)
- `temperature=0`: Respuestas precisas y consistentes (0 = menos creativo, 1 = mÃ¡s creativo)

### CÃ³digo:

```python
print("\nğŸ¤– Inicializando el modelo GPT...")
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
```

---

## PASO 10: Crea la cadena RAG

### Â¿QuÃ© harÃ¡s?

Conectar el modelo GPT con tu base de datos vectorial.

### Â¿Por quÃ©?

AquÃ­ es donde sucede la magia del RAG:

1. Tu pregunta se convierte en vector
2. ChromaDB busca los 3 chunks mÃ¡s similares (`k=3`)
3. GPT recibe tu pregunta + esos 3 chunks como contexto
4. GPT responde basÃ¡ndose en ESE contenido especÃ­fico

### CÃ³digo:

```python
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever(search_kwargs={"k": 3})
)

print("âœ… Â¡Agente RAG listo!")
```

**`chain_type="stuff"` significa que "mete" todos los chunks recuperados en el prompt.**

---

## PASO 11: Loop de preguntas

### Â¿QuÃ© harÃ¡s?

Crear un bucle interactivo donde puedes hacer preguntas ilimitadas.

### Â¿Por quÃ©?

Permite conversar con tu documento de forma natural. Escribe preguntas, obtÃ©n respuestas, repite.

### CÃ³digo:

```python
print("\n" + "="*60)
print("ğŸ‰ Â¡TU AGENTE RAG ESTÃ FUNCIONANDO!")
print("="*60)
print("Escribe 'salir' para terminar\n")

while True:
    pregunta = input("ğŸ’¬ Tu pregunta: ")

    if pregunta.lower() in ['salir', 'exit', 'quit']:
        print("\nğŸ‘‹ Â¡Hasta luego!")
        break

    if not pregunta.strip():
        print("âš ï¸ Por favor escribe una pregunta vÃ¡lida")
        continue

    print("\nğŸ” Buscando en el documento...")
    try:
        respuesta = qa_chain.invoke({"query": pregunta})
        print(f"\nğŸ¤– Respuesta:\n{respuesta['result']}\n")
        print("-" * 60 + "\n")
    except Exception as e:
        print(f"âŒ Error al procesar la pregunta: {e}\n")
```

---

## ğŸ¯ CÃ“DIGO COMPLETO

Crea un archivo `agente_rag.py` y copia todo esto:

```python
import os
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA

# PASO 1: Cargar la API Key
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

if not api_key:
    print("âŒ Error: No se encontrÃ³ OPENAI_API_KEY en el archivo .env")
    exit()

print("âœ… API Key cargada correctamente")

# PASO 2: Cargar el PDF
print("\nğŸ“„ Cargando el PDF...")
ruta_pdf = input("Escribe la ruta de tu PDF: ")

try:
    loader = PyPDFLoader(ruta_pdf)
    documentos = loader.load()
    print(f"âœ… PDF cargado: {len(documentos)} pÃ¡ginas encontradas")
except Exception as e:
    print(f"âŒ Error al cargar el PDF: {e}")
    exit()

# PASO 3: Dividir el texto en chunks
print("\nâœ‚ï¸ Dividiendo el documento en partes mÃ¡s pequeÃ±as...")
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
chunks = text_splitter.split_documents(documentos)
print(f"âœ… Documento dividido en {len(chunks)} chunks")

# PASO 4: Crear embeddings y la base de datos vectorial
print("\nğŸ§  Creando la base de conocimiento...")
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings,
    persist_directory="./chroma_db"
)
print("âœ… Base de conocimiento creada")

# PASO 5: Crear el modelo de lenguaje
print("\nğŸ¤– Inicializando el modelo GPT...")
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

# PASO 6: Crear la cadena RAG
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever(search_kwargs={"k": 3})
)

print("âœ… Â¡Agente RAG listo!")

# PASO 7: Loop de preguntas
print("\n" + "="*60)
print("ğŸ‰ Â¡TU AGENTE RAG ESTÃ FUNCIONANDO!")
print("="*60)
print("Escribe 'salir' para terminar\n")

while True:
    pregunta = input("ğŸ’¬ Tu pregunta: ")

    if pregunta.lower() in ['salir', 'exit', 'quit']:
        print("\nğŸ‘‹ Â¡Hasta luego!")
        break

    if not pregunta.strip():
        print("âš ï¸ Por favor escribe una pregunta vÃ¡lida")
        continue

    print("\nğŸ” Buscando en el documento...")
    try:
        respuesta = qa_chain.invoke({"query": pregunta})
        print(f"\nğŸ¤– Respuesta:\n{respuesta['result']}\n")
        print("-" * 60 + "\n")
    except Exception as e:
        print(f"âŒ Error al procesar la pregunta: {e}\n")
```

---

## â–¶ï¸ Ejecuta tu agente con uv

### OpciÃ³n 1: EjecuciÃ³n directa

```bash
uv run agente_rag.py
```

**uv automÃ¡ticamente:**

- Activa el entorno virtual
- Verifica que todas las dependencias estÃ©n instaladas
- Ejecuta tu script

### OpciÃ³n 2: Ejecutar en el entorno virtual

```bash
# Activa el entorno (si quieres trabajar interactivamente)
# Windows:
.venv\Scripts\activate
# Mac/Linux:
source .venv/bin/activate

# Luego ejecuta normalmente
python agente_rag.py
```

### Ejemplo de uso:

```
âœ… API Key cargada correctamente

ğŸ“„ Cargando el PDF...
Escribe la ruta de tu PDF: manual_python.pdf
âœ… PDF cargado: 10 pÃ¡ginas encontradas

âœ‚ï¸ Dividiendo el documento en partes mÃ¡s pequeÃ±as...
âœ… Documento dividido en 45 chunks

ğŸ§  Creando la base de conocimiento...
âœ… Base de conocimiento creada

ğŸ¤– Inicializando el modelo GPT...
âœ… Â¡Agente RAG listo!

============================================================
ğŸ‰ Â¡TU AGENTE RAG ESTÃ FUNCIONANDO!
============================================================
Escribe 'salir' para terminar

ğŸ’¬ Tu pregunta: Â¿De quÃ© trata este documento?

ğŸ” Buscando en el documento...

ğŸ¤– Respuesta:
Este documento es un manual de Python que cubre los fundamentos del lenguaje...

------------------------------------------------------------

ğŸ’¬ Tu pregunta: salir

ğŸ‘‹ Â¡Hasta luego!
```
