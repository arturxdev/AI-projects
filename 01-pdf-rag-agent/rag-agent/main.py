import os
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain_chroma import Chroma
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.tools import tool
from langchain.chat_models import init_chat_model
from langchain.agents import create_agent

# Cargar variables del archivo .env
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

# Verificar que existe
if not api_key:
    print("âŒ Error: No se encontrÃ³ OPENAI_API_KEY en el archivo .env")
    exit()

print("âœ… API Key cargada correctamente")

# ConfiguraciÃ³n
ruta_pdf = "./pdf prueba.pdf"
persist_directory = "./chroma_db"

# Inicializar embeddings
embeddings = OpenAIEmbeddings()

# Verificar si ya existe la base de datos
if os.path.exists(persist_directory) and os.listdir(persist_directory):
    print("\nâ™»ï¸  Base de conocimiento existente encontrada, cargando...")
    vectorstore = Chroma(
        persist_directory=persist_directory, embedding_function=embeddings
    )
    print("âœ… Base de conocimiento cargada desde disco")
else:
    print("\nğŸ“„ No se encontrÃ³ base de conocimiento, procesando PDF...")
    print(f"ğŸ“„ Cargando el PDF: {ruta_pdf}")

    try:
        loader = PyPDFLoader(ruta_pdf)
        documentos = loader.load()
        print(f"âœ… PDF cargado: {len(documentos)} pÃ¡ginas encontradas")
    except Exception as e:
        print(f"âŒ Error al cargar el PDF: {e}")
        exit()
    print("\nâœ‚ï¸ Dividiendo el documento en partes mÃ¡s pequeÃ±as...")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    chunks = text_splitter.split_documents(documentos)
    print(f"âœ… Documento dividido en {len(chunks)} chunks")

    print("\nğŸ§  Creando la base de conocimiento...")
    vectorstore = Chroma.from_documents(
        documents=chunks, embedding=embeddings, persist_directory=persist_directory
    )
    print("âœ… Base de conocimiento creada y guardada")
    print("\nğŸ¤– Inicializando el modelo GPT...")


@tool(response_format="content_and_artifact")
def retrieve_context(query: str):
    """Retrieve information to help answer a query."""
    retrieved_docs = vectorstore.similarity_search(query, k=2)
    serialized = "\n\n".join(
        (f"Source: {doc.metadata}\nContent: {doc.page_content}")
        for doc in retrieved_docs
    )
    return serialized, retrieved_docs


tools = [retrieve_context]
prompt = (
    "Tienes acceso a una tool que te da informacion de un pdf , responde apartir de esa information "
    "Usa la tool para responder las dudas del usuario, se claro y conciso."
)
model = init_chat_model("gpt-4.1")
agent = create_agent(model, tools, system_prompt=prompt)

query = "Que fue lo que paso con softbank el dia de hoy"

# res = agent.invoke({"messages": [("user", query)]})
# print(res["messages"][0].pretty_print())
# print(res["messages"][-1].pretty_print())


print("ğŸ‰ Â¡TU AGENTE RAG ESTÃ FUNCIONANDO!")
print("Escribe 'salir' para terminar\n")

while True:
    pregunta = input("ğŸ’¬ Tu pregunta: ")

    if pregunta.lower() in ["salir", "exit", "quit"]:
        print("\nğŸ‘‹ Â¡Hasta luego!")
        break

    if not pregunta.strip():
        print("âš ï¸ Por favor escribe una pregunta vÃ¡lida")
        continue

    print("\nğŸ” Buscando en el documento...")
    try:
        respuesta = agent.invoke({"messages": [("user", pregunta)]})
        print(respuesta["messages"][-1].pretty_print())
    except Exception as e:
        print(f"âŒ Error al procesar la pregunta: {e}\n")
