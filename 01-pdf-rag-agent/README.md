# Tu Primer Agente RAG: Construye un Asistente IA que Lee PDFs

## Â¿QuÃ© vas a construir?

Vas a crear tu primer agente de inteligencia artificial capaz de **leer documentos PDF y responder preguntas** sobre su contenido. Es como tener un asistente personal que lee por ti y te explica lo que encuentra.

**Tiempo estimado:** 45-60 minutos

---

## Requisitos previos

âœ… Python 3.10 o superior instalado  
âœ… Un editor de cÃ³digo (VS Code recomendado)  
âœ… API Key de OpenAI ([obtenerla aquÃ­](https://platform.openai.com))  
âœ… Un PDF para probar (cualquier documento)

---

## PASO 1: Instala uv (el manejador de paquetes moderno)

### Â¿QuÃ© es uv?

Es un manejador de paquetes de Python **ultra rÃ¡pido** creado por Astral (los creadores de Ruff). Es hasta 100x mÃ¡s rÃ¡pido que pip tradicional.

### Instala uv

**Windows (PowerShell):**

```powershell
powershell -c "irm https://astral.sh/uv/install.ps1 | iex"
```

**Mac/Linux:**

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

### Verifica la instalaciÃ³n

```bash
uv --version
```

**DeberÃ­as ver algo como: `uv 0.5.x`**

---

## PASO 2: Configura tu proyecto con uv

```bash
# Crea la carpeta del proyecto
mkdir rag-agent
cd rag-agent

# Inicializa el proyecto con uv
uv init
```

![/images/create-project-uv.gif](/images/create-project-uv.gif)

**Esto crea automÃ¡ticamente:**

- Un entorno virtual en `.venv`
- Un archivo `pyproject.toml` para manejar dependencias
- Un archivo `.python-version` con la versiÃ³n de Python

---

## PASO 3: Instala las librerÃ­as necesarias con uv

### Â¿QuÃ© harÃ¡s?

Instalar todas las herramientas que tu agente necesita usando uv.

### Â¿Por quÃ© cada una?

- **langchain**: Framework para construir aplicaciones de IA
- **langchain-openai**: ConexiÃ³n con GPT
- **chromadb**: Base de datos vectorial (el "cerebro" de tu agente)
- **pypdf**: Lector de archivos PDF
- **python-dotenv**: Manejo seguro de tu API Key

### Comando

```bash
uv add chromadb langchain langchain-chroma langchain-community langchain-openai langchain-text-splitters pypdf python-dotenv
```

![Install dependencies](/images/install-dep.gif)

---

## PASO 4: Protege tu API Key

### Â¿QuÃ© harÃ¡s?

Crear un archivo `.env` para guardar tu API Key de forma segura.

### Â¿Por quÃ©?

**Nunca debes compartir tu API Key pÃºblicamente.** Usar `.env` evita que accidentalmente la subas a GitHub.

### Crea o abre .gitignore

pega en ese archivo el siguiente contenido

`.gitignore`

```bash
.env
chroma_db/
.venv
```

![crea gitignore](/images/create-gitignore.gif)

Tu archivo debe verse asi:

### Crea o abre `.env` y agrega

```
OPENAI_API_KEY=tu-api-key-aqui
```

si no sabes como crear tu api key aca te dejo un [video](https://www.youtube.com/watch?v=um4jXio7NjQ)

![env openai](/images/env-openai.gif)

**Guarda el archivo. Tu API Key ahora estÃ¡ protegida.**

---

## PASO 5: Cargar el pdf

Aqui necesitas tu archivo pdf de donde se van hacer las preguntas, puedes tomar mi archivo .

este archivo debe estar posicionado en la raiz del proyecto

### Â¿QuÃ© harÃ¡s?

Crear el archivo `main.py`

### Â¿Por quÃ© cada import?

- `os` y `dotenv`: Para leer la API Key del archivo `.env`
- `OpenAIEmbeddings`: Convierte texto en vectores (nÃºmeros)
- `ChatOpenAI`: El modelo GPT que responderÃ¡ preguntas
- `PyPDFLoader`: Lee archivos PDF
- `RecursiveCharacterTextSplitter`: Divide documentos grandes en pedazos
- `Chroma`: La base de datos vectorial
- `RetrievalQA`: La cadena RAG completa

### CÃ³digo

```python
import os
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain_chroma import Chroma
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.tools import tool
from langchain.chat_models import init_chat_model
from langchain.agents import create_agent

# Cargar variables del archivo .env
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

# Verificar que existe
if not api_key:
    print("âŒ Error: No se encontrÃ³ OPENAI_API_KEY en el archivo .env")
    exit()

print("âœ… API Key cargada correctamente")

# ConfiguraciÃ³n
ruta_pdf = "./pdf prueba.pdf"
persist_directory = "./chroma_db"

# Inicializar embeddings
embeddings = OpenAIEmbeddings()

# Verificar si ya existe la base de datos
if os.path.exists(persist_directory) and os.listdir(persist_directory):
    print("\nâ™»ï¸  Base de conocimiento existente encontrada, cargando...")
    vectorstore = Chroma(
        persist_directory=persist_directory, embedding_function=embeddings
    )
    print("âœ… Base de conocimiento cargada desde disco")
else:
    print("\nğŸ“„ No se encontrÃ³ base de conocimiento, procesando PDF...")
    print(f"ğŸ“„ Cargando el PDF: {ruta_pdf}")

    try:
        loader = PyPDFLoader(ruta_pdf)
        documentos = loader.load()
        print(f"âœ… PDF cargado: {len(documentos)} pÃ¡ginas encontradas")
    except Exception as e:
        print(f"âŒ Error al cargar el PDF: {e}")
        exit()

```

## PASO 6: Divide el texto en chunks

### CÃ³digo

```python
    print("\nâœ‚ï¸ Dividiendo el documento en partes mÃ¡s pequeÃ±as...")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    chunks = text_splitter.split_documents(documentos)
    print(f"âœ… Documento dividido en {len(chunks)} chunks")
```

- `chunk_size=1000`: Cada pedazo tiene mÃ¡ximo 1000 caracteres
- `chunk_overlap=200`: Hay 200 caracteres repetidos entre chunks para no perder contexto

**Un PDF de 5 pÃ¡ginas puede generar 20-30 chunks.**

**PyPDFLoader extrae el texto de cada pÃ¡gina y crea un "documento" por pÃ¡gina.**

---

## PASO 7: Crea embeddings y la base de datos vectorial

```python
    print("\nğŸ§  Creando la base de conocimiento...")
    vectorstore = Chroma.from_documents(
        documents=chunks, embedding=embeddings, persist_directory=persist_directory
    )
    print("âœ… Base de conocimiento creada y guardada")
```

**Esto toma 10-30 segundos. Se crea una carpeta `chroma_db` con tu base de datos.**

---

## PASO 8: Inicializa el modelo GPT

### Â¿Por quÃ© estos parÃ¡metros?

- `model="gpt-3.5-turbo"`: Modelo rÃ¡pido y econÃ³mico (puedes usar gpt-4 si quieres)
- `temperature=0`: Respuestas precisas y consistentes (0 = menos creativo, 1 = mÃ¡s creativo)

### CÃ³digo

```python
print("\nğŸ¤– Inicializando el modelo GPT...")
@tool(response_format="content_and_artifact")
def retrieve_context(query: str):
    """Retrieve information to help answer a query."""
    retrieved_docs = vectorstore.similarity_search(query, k=2)
    serialized = "\n\n".join(
        (f"Source: {doc.metadata}\nContent: {doc.page_content}")
        for doc in retrieved_docs
    )
    return serialized, retrieved_docs


tools = [retrieve_context]
prompt = (
    "Tienes acceso a una tool que te da informacion de un pdf , responde apartir de esa information "
    "Usa la tool para responder las dudas del usuario, se claro y conciso."
)
model = init_chat_model("gpt-4.1")
agent = create_agent(model, tools, system_prompt=prompt)
```

---

## PASO 9: Probar el agente

### Â¿QuÃ© harÃ¡s?

vamos a probar el agente para validar que esta conectado correctamente

```python
query = "Que fue lo que paso con softbank el dia de hoy"

res = agent.invoke({"messages": [("user", query)]})
print(res["messages"][0].pretty_print())
print(res["messages"][-1].pretty_print())
```

Una vez ejecutada la prueba comenta, las tres lÃ­neas anteriores solo era para probar

ejecuta el script para validar el funcionamiento

```shell
uv run main.py
```

Te deberÃ­a dar un resultado como este

```shell
âœ… API Key cargada correctamente

ğŸ“„ No se encontrÃ³ base de conocimiento, procesando PDF...
ğŸ“„ Cargando el PDF: ./pdf prueba.pdf
âœ… PDF cargado: 2 pÃ¡ginas encontradas

âœ‚ï¸  Dividiendo el documento en partes mÃ¡s pequeÃ±as...
âœ… Documento dividido en 8 chunks

ğŸ§  Creando la base de conocimiento...
âœ… Base de conocimiento creada y guardada

ğŸ¤– Inicializando el modelo GPT...
================================ Human Message =================================

Que fue lo que paso con softbank el dia de hoy
None
================================== Ai Message ==================================

Hoy, SoftBank anunciÃ³ la adquisiciÃ³n definitiva de DigitalBridge Group por aproximadamente 4.000 millones de dÃ³lares. Este movimiento estratÃ©gico tiene como principal objetivo escalar la infraestructura de inteligencia artificial de prÃ³xima generaciÃ³n. Con la compra, SoftBank busca expandir su capacidad en centros de datos y conectividad, elementos cruciales para soportar la creciente demanda de cÃ³mputo necesaria para los modelos de lenguaje a gran escala que han dominado el 2025.

Masayoshi Son, lÃ­der de SoftBank, refuerza asÃ­ su apuesta por una "superinteligencia artificial" que requiere una sÃ³lida base fÃ­sica y global para operar sin latencia.
None
```

---

## PASO 10: Loop de preguntas

### Â¿QuÃ© harÃ¡s?

Crear un bucle interactivo donde puedes hacer preguntas ilimitadas.
Para esto comenta, las tres lÃ­neas anteriores solo era para probar

```python
print("ğŸ‰ Â¡TU AGENTE RAG ESTÃ FUNCIONANDO!")
print("Escribe 'salir' para terminar\n")

while True:
    pregunta = input("ğŸ’¬ Tu pregunta: ")

    if pregunta.lower() in ["salir", "exit", "quit"]:
        print("\nğŸ‘‹ Â¡Hasta luego!")
        break

    if not pregunta.strip():
        print("âš ï¸ Por favor escribe una pregunta vÃ¡lida")
        continue

    print("\nğŸ” Buscando en el documento...")
    try:
        respuesta = agent.invoke({"messages": [("user", pregunta)]})
        print(respuesta["messages"][-1].pretty_print())
    except Exception as e:
        print(f"âŒ Error al procesar la pregunta: {e}\n")

```

---

## ğŸ¯ CÃ“DIGO COMPLETO

```python
import os
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader
from langchain_chroma import Chroma
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.tools import tool
from langchain.chat_models import init_chat_model
from langchain.agents import create_agent

# Cargar variables del archivo .env
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

# Verificar que existe
if not api_key:
    print("âŒ Error: No se encontrÃ³ OPENAI_API_KEY en el archivo .env")
    exit()

print("âœ… API Key cargada correctamente")

# ConfiguraciÃ³n
ruta_pdf = "./pdf prueba.pdf"
persist_directory = "./chroma_db"

# Inicializar embeddings
embeddings = OpenAIEmbeddings()

# Verificar si ya existe la base de datos
if os.path.exists(persist_directory) and os.listdir(persist_directory):
    print("\nâ™»ï¸  Base de conocimiento existente encontrada, cargando...")
    vectorstore = Chroma(
        persist_directory=persist_directory, embedding_function=embeddings
    )
    print("âœ… Base de conocimiento cargada desde disco")
else:
    print("\nğŸ“„ No se encontrÃ³ base de conocimiento, procesando PDF...")
    print(f"ğŸ“„ Cargando el PDF: {ruta_pdf}")

    try:
        loader = PyPDFLoader(ruta_pdf)
        documentos = loader.load()
        print(f"âœ… PDF cargado: {len(documentos)} pÃ¡ginas encontradas")
    except Exception as e:
        print(f"âŒ Error al cargar el PDF: {e}")
        exit()

    print("\nâœ‚ï¸ Dividiendo el documento en partes mÃ¡s pequeÃ±as...")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    chunks = text_splitter.split_documents(documentos)
    print(f"âœ… Documento dividido en {len(chunks)} chunks")

    print("\nğŸ§  Creando la base de conocimiento...")
    vectorstore = Chroma.from_documents(
        documents=chunks, embedding=embeddings, persist_directory=persist_directory
    )
    print("âœ… Base de conocimiento creada y guardada")


print("\nğŸ¤– Inicializando el modelo GPT...")
@tool(response_format="content_and_artifact")
def retrieve_context(query: str):
    """Retrieve information to help answer a query."""
    retrieved_docs = vectorstore.similarity_search(query, k=2)
    serialized = "\n\n".join(
        (f"Source: {doc.metadata}\nContent: {doc.page_content}")
        for doc in retrieved_docs
    )
    return serialized, retrieved_docs


tools = [retrieve_context]
prompt = (
    "Tienes acceso a una tool que te da informacion de un pdf , responde apartir de esa information "
    "Usa la tool para responder las dudas del usuario, se claro y conciso."
)
model = init_chat_model("gpt-4.1")
agent = create_agent(model, tools, system_prompt=prompt)


query = "Que fue lo que paso con softbank el dia de hoy"

res = agent.invoke({"messages": [("user", query)]})
print(res["messages"][0].pretty_print())
print(res["messages"][-1].pretty_print())


print("ğŸ‰ Â¡TU AGENTE RAG ESTÃ FUNCIONANDO!")
print("Escribe 'salir' para terminar\n")

while True:
    pregunta = input("ğŸ’¬ Tu pregunta: ")

    if pregunta.lower() in ["salir", "exit", "quit"]:
        print("\nğŸ‘‹ Â¡Hasta luego!")
        break

    if not pregunta.strip():
        print("âš ï¸ Por favor escribe una pregunta vÃ¡lida")
        continue

    print("\nğŸ” Buscando en el documento...")
    try:
        respuesta = agent.invoke({"messages": [("user", pregunta)]})
        print(respuesta["messages"][-1].pretty_print())
    except Exception as e:
        print(f"âŒ Error al procesar la pregunta: {e}\n")
```

---

## â–¶ï¸ Ejecuta tu agente con uv

```bash
uv run main.py
```

### Ejemplo de uso

```
âœ… API Key cargada correctamente

â™»ï¸   Base de conocimiento existente encontrada, cargando...
âœ… Base de conocimiento cargada desde disco

ğŸ¤– Inicializando el modelo GPT...
ğŸ‰ Â¡TU AGENTE RAG ESTÃ FUNCIONANDO!
Escribe 'salir' para terminar

ğŸ’¬ Tu pregunta: que compro nvidia?

ğŸ” Buscando en el documento...
================================== Ai Message ==================================

Nvidia comprÃ³ una participaciÃ³n superior al 4% en Intel, por la que desembolsÃ³ 5.000 millones de dÃ³lares. Esta operaciÃ³n forma parte de una reestructuraciÃ³n financiera respaldada por SoftBank y el gobierno de EE. UU. La alianza tÃ©cnica y financiera entre ambas empresas busca asegurar que Intel continÃºe fabricando semiconductores de vanguardia, mientras que Nvidia garantiza una cadena de suministro de chips de inferencia mÃ¡s estable.
None
ğŸ’¬ Tu pregunta: salir

ğŸ‘‹ Â¡Hasta luego!
```
